{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaline and Gradient Descent\n",
    "\n",
    "In 1960, Bernard Widrow and Ted Hoff published Adaline (Adaptive Linear Neuron), which introduced a key improvement over the perceptron: using gradient descent to minimize a continuous loss function.\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Understand the limitation of the perceptron update rule\n",
    "2. Learn about loss functions and gradient descent\n",
    "3. Implement Adaline from scratch\n",
    "4. See why feature scaling matters\n",
    "5. Compare batch gradient descent vs stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem with Perceptron\n",
    "\n",
    "The perceptron update rule uses the **thresholded output** (0 or 1):\n",
    "\n",
    "$$w = w + \\eta (y - \\hat{y}) x$$\n",
    "\n",
    "This gives us only binary feedback: right or wrong. We never know **how wrong** we are.\n",
    "\n",
    "- Missing by 0.001? Wrong.\n",
    "- Missing by 1000? Wrong.\n",
    "\n",
    "Same update either way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaline: The Key Insight\n",
    "\n",
    "Adaline updates weights based on the **continuous net input** instead of the thresholded output.\n",
    "\n",
    "| | Perceptron | Adaline |\n",
    "|---|---|---|\n",
    "| Computes | z = w * x + b | z = w * x + b |\n",
    "| Updates on | step(z) | z directly |\n",
    "| Error signal | Binary (0 or 1) | Continuous |\n",
    "\n",
    "The threshold is only applied for the final prediction, not during learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Loss Function: Mean Squared Error\n",
    "\n",
    "To measure \"how wrong\" we are, we define a **loss function**:\n",
    "\n",
    "$$L(w, b) = \\frac{1}{n} \\sum_{i=1}^{n} (y^{(i)} - z^{(i)})^2$$\n",
    "\n",
    "Where $z^{(i)} = w \\cdot x^{(i)} + b$ is the continuous net input.\n",
    "\n",
    "**Why squared error?**\n",
    "- Positive and negative errors do not cancel out\n",
    "- Larger errors are penalized more heavily\n",
    "- Differentiable everywhere (crucial for gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "The gradient tells us which direction is \"uphill\" in weight space. To minimize loss, we move in the **opposite** direction:\n",
    "\n",
    "$$w = w - \\eta \\nabla L$$\n",
    "\n",
    "For MSE loss, the gradient is:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_j} = -\\frac{2}{n} \\sum_{i=1}^{n} (y^{(i)} - z^{(i)}) x_j^{(i)}$$\n",
    "\n",
    "So the update rule becomes:\n",
    "\n",
    "$$w = w + \\eta \\frac{2}{n} \\sum_{i=1}^{n} (y^{(i)} - z^{(i)}) x^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation: AdalineGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdalineGD:\n",
    "    \"\"\"Adaline classifier using batch gradient descent.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    eta : float\n",
    "        Learning rate (between 0.0 and 1.0)\n",
    "    n_iter : int\n",
    "        Number of passes over the training dataset (epochs)\n",
    "    random_state : int\n",
    "        Random seed for weight initialization\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    w_ : 1d-array\n",
    "        Weights after fitting\n",
    "    b_ : float\n",
    "        Bias unit after fitting\n",
    "    losses_ : list\n",
    "        MSE loss in each epoch\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit training data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_examples, n_features]\n",
    "            Training vectors\n",
    "        y : array-like, shape = [n_examples]\n",
    "            Target values\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1])\n",
    "        self.b_ = np.float64(0.0)\n",
    "        self.losses_ = []\n",
    "        \n",
    "        for _ in range(self.n_iter):\n",
    "            net_input = self.net_input(X)\n",
    "            output = self.activation(net_input)\n",
    "            errors = y - output\n",
    "            \n",
    "            self.w_ += self.eta * 2.0 * X.T.dot(errors) / X.shape[0]\n",
    "            self.b_ += self.eta * 2.0 * errors.mean()\n",
    "            \n",
    "            loss = (errors ** 2).mean()\n",
    "            self.losses_.append(loss)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate net input: w . x + b\"\"\"\n",
    "        return np.dot(X, self.w_) + self.b_\n",
    "    \n",
    "    def activation(self, X):\n",
    "        \"\"\"Compute linear activation (identity function)\"\"\"\n",
    "        return X\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "df = pd.read_csv(url, header=None, encoding='utf-8')\n",
    "\n",
    "y = df.iloc[0:100, 4].values\n",
    "y = np.where(y == 'Iris-setosa', 0, 1)\n",
    "\n",
    "X = df.iloc[0:100, [0, 2]].values\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Learning Rate Problem\n",
    "\n",
    "Let's see what happens with different learning rates on **unstandardized** data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "\n",
    "ada1 = AdalineGD(eta=0.1, n_iter=15).fit(X, y)\n",
    "ax[0].plot(range(1, len(ada1.losses_) + 1), np.log10(ada1.losses_), marker='o')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('log(MSE)')\n",
    "ax[0].set_title('Adaline - Learning rate 0.1')\n",
    "\n",
    "ada2 = AdalineGD(eta=0.0001, n_iter=15).fit(X, y)\n",
    "ax[1].plot(range(1, len(ada2.losses_) + 1), ada2.losses_, marker='o')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('MSE')\n",
    "ax[1].set_title('Adaline - Learning rate 0.0001')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Too large (0.1):** Loss explodes! We overshoot the minimum and diverge.\n",
    "\n",
    "**Too small (0.0001):** Loss decreases, but very slowly. Would need many more epochs.\n",
    "\n",
    "The problem is that our features have very different scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling: Standardization\n",
    "\n",
    "Standardization transforms each feature to have mean=0 and std=1:\n",
    "\n",
    "$$x' = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "This makes the loss surface more \"spherical\" so gradient descent can use a consistent step size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std = np.copy(X)\n",
    "X_std[:, 0] = (X[:, 0] - X[:, 0].mean()) / X[:, 0].std()\n",
    "X_std[:, 1] = (X[:, 1] - X[:, 1].mean()) / X[:, 1].std()\n",
    "\n",
    "print(f\"Original X - Feature 0: mean={X[:, 0].mean():.2f}, std={X[:, 0].std():.2f}\")\n",
    "print(f\"Original X - Feature 1: mean={X[:, 1].mean():.2f}, std={X[:, 1].std():.2f}\")\n",
    "print()\n",
    "print(f\"Standardized X - Feature 0: mean={X_std[:, 0].mean():.2f}, std={X_std[:, 0].std():.2f}\")\n",
    "print(f\"Standardized X - Feature 1: mean={X_std[:, 1].mean():.2f}, std={X_std[:, 1].std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_std = AdalineGD(eta=0.5, n_iter=20).fit(X_std, y)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(ada_std.losses_) + 1), ada_std.losses_, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Adaline with Standardized Features (eta=0.5)')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final MSE: {ada_std.losses_[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With standardized features, we can use a much larger learning rate (0.5) and converge quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
    "    markers = ('o', 's', '^', 'v', '<')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    \n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(\n",
    "        np.arange(x1_min, x1_max, resolution),\n",
    "        np.arange(x2_min, x2_max, resolution)\n",
    "    )\n",
    "    \n",
    "    lab = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    lab = lab.reshape(xx1.shape)\n",
    "    \n",
    "    plt.contourf(xx1, xx2, lab, alpha=0.3, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "    \n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(\n",
    "            x=X[y == cl, 0],\n",
    "            y=X[y == cl, 1],\n",
    "            alpha=0.8,\n",
    "            c=colors[idx],\n",
    "            marker=markers[idx],\n",
    "            label=f'Class {cl}',\n",
    "            edgecolor='black'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plot_decision_regions(X_std, y, classifier=ada_std)\n",
    "plt.xlabel('Sepal length [standardized]')\n",
    "plt.ylabel('Petal length [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Adaline Decision Boundary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = ada_std.predict(X_std)\n",
    "accuracy = np.mean(predictions == y)\n",
    "print(f\"Training accuracy: {accuracy * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Batch gradient descent computes the gradient over **all** samples before updating. This can be slow for large datasets.\n",
    "\n",
    "**Stochastic Gradient Descent** updates weights after **each** sample:\n",
    "\n",
    "- Faster iterations\n",
    "- Noisier updates (can help escape local minima)\n",
    "- Enables online learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdalineSGD:\n",
    "    \"\"\"Adaline classifier using stochastic gradient descent.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    eta : float\n",
    "        Learning rate (between 0.0 and 1.0)\n",
    "    n_iter : int\n",
    "        Number of passes over the training dataset (epochs)\n",
    "    shuffle : bool\n",
    "        Shuffle training data each epoch to prevent cycles\n",
    "    random_state : int\n",
    "        Random seed for weight initialization and shuffling\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, eta=0.01, n_iter=10, shuffle=True, random_state=None):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "        self.w_initialized = False\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self._initialize_weights(X.shape[1])\n",
    "        self.losses_ = []\n",
    "        \n",
    "        for _ in range(self.n_iter):\n",
    "            if self.shuffle:\n",
    "                X, y = self._shuffle(X, y)\n",
    "            \n",
    "            losses = []\n",
    "            for xi, target in zip(X, y):\n",
    "                losses.append(self._update_weights(xi, target))\n",
    "            \n",
    "            avg_loss = np.mean(losses)\n",
    "            self.losses_.append(avg_loss)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def partial_fit(self, X, y):\n",
    "        \"\"\"Fit training data without reinitializing weights (for online learning).\"\"\"\n",
    "        if not self.w_initialized:\n",
    "            self._initialize_weights(X.shape[1])\n",
    "        \n",
    "        if y.ravel().shape[0] > 1:\n",
    "            for xi, target in zip(X, y):\n",
    "                self._update_weights(xi, target)\n",
    "        else:\n",
    "            self._update_weights(X, y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _initialize_weights(self, m):\n",
    "        self.rgen = np.random.RandomState(self.random_state)\n",
    "        self.w_ = self.rgen.normal(loc=0.0, scale=0.01, size=m)\n",
    "        self.b_ = np.float64(0.0)\n",
    "        self.w_initialized = True\n",
    "    \n",
    "    def _shuffle(self, X, y):\n",
    "        r = self.rgen.permutation(len(y))\n",
    "        return X[r], y[r]\n",
    "    \n",
    "    def _update_weights(self, xi, target):\n",
    "        output = self.activation(self.net_input(xi))\n",
    "        error = target - output\n",
    "        self.w_ += self.eta * 2.0 * xi * error\n",
    "        self.b_ += self.eta * 2.0 * error\n",
    "        loss = error ** 2\n",
    "        return loss\n",
    "    \n",
    "    def net_input(self, X):\n",
    "        return np.dot(X, self.w_) + self.b_\n",
    "    \n",
    "    def activation(self, X):\n",
    "        return X\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_sgd = AdalineSGD(eta=0.01, n_iter=15, random_state=1)\n",
    "ada_sgd.fit(X_std, y)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(ada_sgd.losses_) + 1), ada_sgd.losses_, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average MSE')\n",
    "plt.title('Adaline SGD Convergence')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training accuracy: {np.mean(ada_sgd.predict(X_std) == y) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Learning with partial_fit\n",
    "\n",
    "SGD enables **online learning**: updating the model as new data arrives without retraining from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_online = AdalineSGD(eta=0.01, random_state=1)\n",
    "\n",
    "for i in range(0, 100, 10):\n",
    "    X_batch = X_std[i:i+10]\n",
    "    y_batch = y[i:i+10]\n",
    "    ada_online.partial_fit(X_batch, y_batch)\n",
    "    acc = np.mean(ada_online.predict(X_std) == y)\n",
    "    print(f\"After samples {i+1}-{i+10}: accuracy = {acc*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Batch vs SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "\n",
    "ada_gd = AdalineGD(eta=0.5, n_iter=20).fit(X_std, y)\n",
    "ax[0].plot(range(1, len(ada_gd.losses_) + 1), ada_gd.losses_, marker='o')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('MSE')\n",
    "ax[0].set_title('Batch Gradient Descent')\n",
    "\n",
    "ada_sgd2 = AdalineSGD(eta=0.01, n_iter=20, random_state=1).fit(X_std, y)\n",
    "ax[1].plot(range(1, len(ada_sgd2.losses_) + 1), ada_sgd2.losses_, marker='o')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Average MSE')\n",
    "ax[1].set_title('Stochastic Gradient Descent')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch GD:** Smooth convergence, but computes gradient over all 100 samples each epoch.\n",
    "\n",
    "**SGD:** Noisier convergence, but updates 100 times per epoch. Scales better to large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Learning rate exploration:** Try eta values from 0.001 to 1.0 on standardized data. Plot the loss curves.\n",
    "\n",
    "2. **Without standardization:** Train Adaline on raw (non-standardized) data. What learning rate works?\n",
    "\n",
    "3. **Mini-batch SGD:** Modify AdalineSGD to update weights on mini-batches of 16 samples instead of 1.\n",
    "\n",
    "4. **Compare with Perceptron:** Train both Perceptron and Adaline on the same data. Compare convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Learning rate exploration\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Without standardization\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Mini-batch SGD\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Compare with Perceptron\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Concept | What It Does |\n",
    "|---------|-------------|\n",
    "| MSE Loss | Measures how wrong we are (continuously) |\n",
    "| Gradient | Points uphill in weight space |\n",
    "| Gradient Descent | Steps downhill to minimize loss |\n",
    "| Learning Rate | Controls step size |\n",
    "| Standardization | Makes optimization easier |\n",
    "| SGD | Scales to large datasets |\n",
    "\n",
    "**The key insight:** By using a continuous loss function and gradient descent, we get smooth optimization that tells us not just whether we are wrong, but **how wrong** and in **which direction** to improve.\n",
    "\n",
    "This is the foundation of all deep learning. Every neural network, from simple MLPs to GPT, is trained using gradient descent on a loss function.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next session, we will cover:\n",
    "- **Multi-Layer Perceptrons:** Adding hidden layers to solve non-linear problems\n",
    "- **Backpropagation:** How gradients flow through multiple layers\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Interactive Demo](https://i33ym.cc/demo-adaline/)\n",
    "- [Full Essay](https://i33ym.cc/the-adaline/)\n",
    "- [Slides](https://i33ym.cc/slides-adaline/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
