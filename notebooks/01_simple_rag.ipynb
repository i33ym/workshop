{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UShF8D3xLWI0"
      },
      "source": [
        "# Part 1: Simple RAG\n",
        "\n",
        "In this notebook, we'll build a basic RAG system and see why it doesn't work well.\n",
        "\n",
        "**What is RAG?**\n",
        "- **R**etrieval — find relevant documents\n",
        "- **A**ugmented — add them to the AI's context\n",
        "- **G**eneration — generate an answer\n",
        "\n",
        "By the end, you'll understand why simple RAG only achieves ~30% accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RoFY1D0LWI2"
      },
      "source": [
        "## Step 0: Setup\n",
        "\n",
        "Run this cell first to clone the repo and install dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFRCI3SILWI3"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/i33ym/workshop.git 2>/dev/null || echo \"Already cloned\"\n",
        "%cd workshop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-oqoG4jLWI4"
      },
      "outputs": [],
      "source": [
        "!pip install -q openai langchain langchain-openai langchain-community chromadb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XW3lPQzLWI5"
      },
      "source": [
        "## Step 1: Set Your API Key\n",
        "\n",
        "Get your OpenAI API key from [platform.openai.com](https://platform.openai.com/api-keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioJbNQYELWI6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8_qmQ6lLWI7"
      },
      "source": [
        "## Step 2: Load the Documents\n",
        "\n",
        "We'll load markdown files from the `docs/` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgb5HBvdLWI8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
        "\n",
        "loader = DirectoryLoader(\n",
        "    \"docs/\",\n",
        "    glob=\"**/*.md\",\n",
        "    loader_cls=TextLoader\n",
        ")\n",
        "\n",
        "documents = loader.load()\n",
        "print(f\"Loaded {len(documents)} documents\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UCc1kZ6LWI9"
      },
      "source": [
        "## Step 3: Split Into Chunks\n",
        "\n",
        "Documents are too long to process at once. We split them into smaller chunks.\n",
        "\n",
        "**Why chunking matters:**\n",
        "- LLMs have context limits\n",
        "- Smaller chunks = more precise retrieval\n",
        "- But too small = losing context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ug773UeMLWI-"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "\n",
        "chunks = splitter.split_documents(documents)\n",
        "print(f\"Split into {len(chunks)} chunks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-TOgwuILWI-"
      },
      "outputs": [],
      "source": [
        "# Let's look at one chunk\n",
        "print(\"=== Sample Chunk ===\")\n",
        "print(chunks[0].page_content[:500])\n",
        "print(\"\\n=== Metadata ===\")\n",
        "print(chunks[0].metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx9pW3lLLWI-"
      },
      "source": [
        "## Step 4: Create Embeddings\n",
        "\n",
        "**What are embeddings?**\n",
        "\n",
        "Embeddings convert text into numbers (vectors) that capture meaning.\n",
        "\n",
        "Similar texts have similar vectors. This lets us find relevant documents by comparing vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYIbWe48LWI_"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# Test it\n",
        "test_embedding = embeddings.embed_query(\"How do I authenticate?\")\n",
        "print(f\"Embedding dimension: {len(test_embedding)}\")\n",
        "print(f\"First 5 values: {test_embedding[:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzGZqsbJLWI_"
      },
      "source": [
        "## Step 5: Create Vector Store\n",
        "\n",
        "A vector store holds all our chunk embeddings and lets us search by similarity.\n",
        "\n",
        "We'll use ChromaDB (runs in memory, no setup needed)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iToBAYt1LWI_"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "vector_store = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embeddings\n",
        ")\n",
        "\n",
        "print(f\"Vector store created with {len(chunks)} chunks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MbCJyD2LWI_"
      },
      "source": [
        "## Step 6: Test Retrieval\n",
        "\n",
        "Let's search for relevant documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhHvAHOTLWI_"
      },
      "outputs": [],
      "source": [
        "query = \"How do I get an authorization token?\"\n",
        "\n",
        "results = vector_store.similarity_search(query, k=3)\n",
        "\n",
        "print(f\"Query: {query}\\n\")\n",
        "for i, doc in enumerate(results):\n",
        "    print(f\"=== Result {i+1} ===\")\n",
        "    print(doc.page_content[:300])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMMRAMqcLWJA"
      },
      "source": [
        "## Step 7: Build Simple RAG\n",
        "\n",
        "Now let's combine retrieval with generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwxpR03LLWJA"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "Answer the question based only on the following context.\n",
        "If you can't find the answer, say \"I don't know.\"\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\")\n",
        "\n",
        "def simple_rag(question):\n",
        "    # Retrieve\n",
        "    docs = vector_store.similarity_search(question, k=3)\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "    # Generate\n",
        "    chain = prompt | llm | StrOutputParser()\n",
        "    answer = chain.invoke({\"context\": context, \"question\": question})\n",
        "\n",
        "    return answer, docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNqNpIdFLWJB"
      },
      "outputs": [],
      "source": [
        "# Test it!\n",
        "question = \"How do I get an authorization token?\"\n",
        "\n",
        "answer, docs = simple_rag(question)\n",
        "\n",
        "print(f\"Question: {question}\\n\")\n",
        "print(f\"Answer: {answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6VTzyT5LWJB"
      },
      "source": [
        "## Step 8: Test More Questions\n",
        "\n",
        "Let's see how well it performs on different types of questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVrJTENfLWJC"
      },
      "outputs": [],
      "source": [
        "test_questions = [\n",
        "    \"How do I create a payment?\",\n",
        "    \"What error codes can the API return?\",\n",
        "    \"How do I set up webhooks?\",\n",
        "    \"What is the endpoint for checking payment status?\",\n",
        "    \"How do I authenticate API requests?\"\n",
        "]\n",
        "\n",
        "for q in test_questions:\n",
        "    answer, _ = simple_rag(q)\n",
        "    print(f\"Q: {q}\")\n",
        "    print(f\"A: {answer[:200]}...\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOSDfUL7LWJC"
      },
      "source": [
        "## Problems with Simple RAG\n",
        "\n",
        "You probably noticed some issues:\n",
        "\n",
        "### 1. Retrieval misses exact terms\n",
        "Vector search is semantic — it finds similar *meanings*, not exact *words*.\n",
        "\n",
        "If you search for `POST /api/v1/payment`, you might get docs about \"creating payments\" instead of the actual endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yra1CzXFLWJC"
      },
      "outputs": [],
      "source": [
        "# Try an exact endpoint search\n",
        "results = vector_store.similarity_search(\"POST /api/payment/create\", k=3)\n",
        "\n",
        "print(\"Searching for exact endpoint 'POST /api/payment/create':\\n\")\n",
        "for i, doc in enumerate(results):\n",
        "    print(f\"Result {i+1}: {doc.page_content[:150]}...\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhtpGsQoLWJC"
      },
      "source": [
        "### 2. No relevance verification\n",
        "Even if documents aren't really relevant, we still generate an answer from them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wrn8HT1MLWJD"
      },
      "outputs": [],
      "source": [
        "# Ask about something NOT in the docs\n",
        "answer, docs = simple_rag(\"How do I integrate with Stripe?\")\n",
        "\n",
        "print(f\"Question about something NOT in docs:\\n\")\n",
        "print(f\"Answer: {answer}\")\n",
        "print(f\"\\n(Notice: it might hallucinate or give wrong info)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIbt1mExLWJD"
      },
      "source": [
        "### 3. All retrieved docs are used equally\n",
        "Some retrieved documents are more relevant than others, but we treat them all the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kI7Phm2qLWJD"
      },
      "outputs": [],
      "source": [
        "# Look at similarity scores\n",
        "results_with_scores = vector_store.similarity_search_with_score(\"How do I authenticate?\", k=5)\n",
        "\n",
        "print(\"Similarity scores (lower = more similar):\\n\")\n",
        "for doc, score in results_with_scores:\n",
        "    print(f\"Score: {score:.3f} | {doc.page_content[:60]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We4b97VxLWJD"
      },
      "source": [
        "## Benchmark Results\n",
        "\n",
        "Research comparing 18 RAG techniques found:\n",
        "\n",
        "| Technique | Accuracy |\n",
        "|-----------|----------|\n",
        "| **Simple RAG** | **0.30** |\n",
        "| Semantic Chunking | 0.20 |\n",
        "| HyDE | 0.50 |\n",
        "| Reranker | 0.70 |\n",
        "| Hybrid Search | 0.83 |\n",
        "| Adaptive RAG | 0.86 |\n",
        "\n",
        "Simple RAG only gets 30% right. We can do much better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo_mHRIwLWJE"
      },
      "source": [
        "## Summary\n",
        "\n",
        "**What we built:**\n",
        "- Loaded documents\n",
        "- Split into chunks\n",
        "- Created embeddings\n",
        "- Built a vector store\n",
        "- Combined retrieval + generation\n",
        "\n",
        "**Why it's not enough:**\n",
        "- Vector search misses exact matches\n",
        "- No relevance verification\n",
        "- No reranking of results\n",
        "- No hallucination prevention\n",
        "\n",
        "**Next notebook:** We'll fix all of these problems and build a production-ready system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25ScJE2BLWJE"
      },
      "outputs": [],
      "source": [
        "print(\"Part 1 complete!\")\n",
        "print(\"Next: 02_production_rag.ipynb\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}