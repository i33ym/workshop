{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Perceptron: Where It All Began\n",
    "\n",
    "In 1957, Frank Rosenblatt published the first concept of the perceptron learning rule. It was the first algorithm that could learn from examples.\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Understand what machine learning actually is\n",
    "2. Implement a perceptron from scratch\n",
    "3. Train it on real data (Iris dataset)\n",
    "4. Visualize the decision boundary\n",
    "5. See where it fails (XOR problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Machine Learning?\n",
    "\n",
    "**Traditional programming:**\n",
    "```\n",
    "Rules + Data → Output\n",
    "```\n",
    "\n",
    "**Machine learning:**\n",
    "```\n",
    "Data + Output → Rules\n",
    "```\n",
    "\n",
    "Instead of writing rules by hand, we let the computer learn them from examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Perceptron Model\n",
    "\n",
    "The perceptron mimics a biological neuron:\n",
    "\n",
    "1. Take inputs (x)\n",
    "2. Multiply each by a weight (w)\n",
    "3. Add them up, plus a bias (b)\n",
    "4. If the sum >= 0, output 1. Otherwise, output 0.\n",
    "\n",
    "**Net input:**\n",
    "$$z = w_1 x_1 + w_2 x_2 + ... + w_m x_m + b = \\mathbf{w} \\cdot \\mathbf{x} + b$$\n",
    "\n",
    "**Decision function (unit step):**\n",
    "$$\\hat{y} = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ 0 & \\text{otherwise} \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Let's build it step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\"Perceptron classifier.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    eta : float\n",
    "        Learning rate (between 0.0 and 1.0)\n",
    "    n_iter : int\n",
    "        Number of passes over the training dataset (epochs)\n",
    "    random_state : int\n",
    "        Random seed for weight initialization\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    w_ : 1d-array\n",
    "        Weights after fitting\n",
    "    b_ : float\n",
    "        Bias unit after fitting\n",
    "    errors_ : list\n",
    "        Number of misclassifications in each epoch\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit training data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_examples, n_features]\n",
    "            Training vectors\n",
    "        y : array-like, shape = [n_examples]\n",
    "            Target values (0 or 1)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1])\n",
    "        self.b_ = np.float64(0.0)\n",
    "        self.errors_ = []\n",
    "        \n",
    "        for _ in range(self.n_iter):\n",
    "            errors = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                update = self.eta * (target - self.predict(xi))\n",
    "                self.w_ += update * xi\n",
    "                self.b_ += update\n",
    "                errors += int(update != 0.0)\n",
    "            self.errors_.append(errors)\n",
    "            \n",
    "            # Early stopping if converged\n",
    "            if errors == 0:\n",
    "                break\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate net input: w . x + b\"\"\"\n",
    "        return np.dot(X, self.w_) + self.b_\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        return np.where(self.net_input(X) >= 0.0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Learning Rule\n",
    "\n",
    "The update rule is simple:\n",
    "\n",
    "$$w = w + \\eta (y - \\hat{y}) x$$\n",
    "$$b = b + \\eta (y - \\hat{y})$$\n",
    "\n",
    "Where:\n",
    "- $\\eta$ is the learning rate\n",
    "- $y$ is the true label\n",
    "- $\\hat{y}$ is the predicted label\n",
    "\n",
    "**When correct:** $(y - \\hat{y}) = 0$ → no update\n",
    "\n",
    "**When wrong:**\n",
    "- Predicted 0, actual 1 → $(y - \\hat{y}) = +1$ → weights move toward the input\n",
    "- Predicted 1, actual 0 → $(y - \\hat{y}) = -1$ → weights move away from the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on the Iris Dataset\n",
    "\n",
    "The classic machine learning dataset: 150 flower samples, 3 species, 4 features.\n",
    "\n",
    "We will use:\n",
    "- 2 species: setosa and versicolor (first 100 samples)\n",
    "- 2 features: sepal length and petal length (for visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "df = pd.read_csv(url, header=None, encoding='utf-8')\n",
    "\n",
    "# Show first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract setosa and versicolor (first 100 samples)\n",
    "y = df.iloc[0:100, 4].values\n",
    "y = np.where(y == 'Iris-setosa', 0, 1)\n",
    "\n",
    "# Extract sepal length (column 0) and petal length (column 2)\n",
    "X = df.iloc[0:100, [0, 2]].values\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"Classes: {np.unique(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:50, 0], X[:50, 1], color='red', marker='o', label='Setosa')\n",
    "plt.scatter(X[50:100, 0], X[50:100, 1], color='blue', marker='s', label='Versicolor')\n",
    "plt.xlabel('Sepal length [cm]')\n",
    "plt.ylabel('Petal length [cm]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Iris Dataset: Setosa vs Versicolor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two classes are clearly separable by a line. This is called **linear separability**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the perceptron\n",
    "ppn = Perceptron(eta=0.1, n_iter=10)\n",
    "ppn.fit(X, y)\n",
    "\n",
    "print(f\"Errors per epoch: {ppn.errors_}\")\n",
    "print(f\"Final weights: {ppn.w_}\")\n",
    "print(f\"Final bias: {ppn.b_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot convergence\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Number of misclassifications')\n",
    "plt.title('Perceptron Convergence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron converges to zero errors, meaning it found a line that perfectly separates the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
    "    \"\"\"Plot decision regions for a 2D dataset.\"\"\"\n",
    "    # Setup marker generator and color map\n",
    "    markers = ('o', 's', '^', 'v', '<')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    \n",
    "    # Plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(\n",
    "        np.arange(x1_min, x1_max, resolution),\n",
    "        np.arange(x2_min, x2_max, resolution)\n",
    "    )\n",
    "    \n",
    "    lab = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    lab = lab.reshape(xx1.shape)\n",
    "    \n",
    "    plt.contourf(xx1, xx2, lab, alpha=0.3, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "    \n",
    "    # Plot class examples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(\n",
    "            x=X[y == cl, 0],\n",
    "            y=X[y == cl, 1],\n",
    "            alpha=0.8,\n",
    "            c=colors[idx],\n",
    "            marker=markers[idx],\n",
    "            label=f'Class {cl}',\n",
    "            edgecolor='black'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plot_decision_regions(X, y, classifier=ppn)\n",
    "plt.xlabel('Sepal length [cm]')\n",
    "plt.ylabel('Petal length [cm]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Perceptron Decision Boundary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = ppn.predict(X)\n",
    "accuracy = np.mean(predictions == y)\n",
    "print(f\"Training accuracy: {accuracy * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Limitation: XOR Problem\n",
    "\n",
    "The perceptron can only solve **linearly separable** problems. Here is a classic example where it fails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR dataset\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([0, 1, 1, 0])  # XOR: same inputs → 0, different inputs → 1\n",
    "\n",
    "print(\"XOR Truth Table:\")\n",
    "print(\"x1  x2  |  y\")\n",
    "print(\"-\" * 12)\n",
    "for (x1, x2), y_val in zip(X_xor, y_xor):\n",
    "    print(f\" {x1}   {x2}  |  {y_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize XOR\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X_xor[y_xor == 0, 0], X_xor[y_xor == 0, 1], \n",
    "            color='red', marker='o', s=200, label='Class 0')\n",
    "plt.scatter(X_xor[y_xor == 1, 0], X_xor[y_xor == 1, 1], \n",
    "            color='blue', marker='s', s=200, label='Class 1')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.legend()\n",
    "plt.title('XOR Problem: Try drawing a single line to separate the classes')\n",
    "plt.xlim(-0.5, 1.5)\n",
    "plt.ylim(-0.5, 1.5)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to train perceptron on XOR\n",
    "ppn_xor = Perceptron(eta=0.1, n_iter=100)\n",
    "ppn_xor.fit(X_xor, y_xor)\n",
    "\n",
    "print(f\"Errors per epoch (last 10): {ppn_xor.errors_[-10:]}\")\n",
    "print(f\"Final accuracy: {np.mean(ppn_xor.predict(X_xor) == y_xor) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron never converges on XOR. It keeps making errors because no single line can separate the classes.\n",
    "\n",
    "This limitation was pointed out by Minsky and Papert in 1969, which led to the first \"AI Winter.\"\n",
    "\n",
    "**The solution:** Add more layers (multi-layer perceptron)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Try different learning rates:** What happens with eta=0.001 vs eta=1.0?\n",
    "\n",
    "2. **Use different features:** Try sepal width and petal width instead.\n",
    "\n",
    "3. **Versicolor vs Virginica:** Use samples 50-150 instead. Does the perceptron converge? Why or why not?\n",
    "\n",
    "4. **Implement from scratch:** Without looking at the code above, write your own `predict` and `fit` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Try different learning rates\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Use different features\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Versicolor vs Virginica\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The perceptron is just:\n",
    "\n",
    "```python\n",
    "if dot_product + bias >= 0:\n",
    "    return 1\n",
    "else:\n",
    "    return 0\n",
    "```\n",
    "\n",
    "But it introduced every key idea we still use today:\n",
    "- Weighted sums\n",
    "- Activation functions\n",
    "- Bias terms\n",
    "- Iterative learning from examples\n",
    "\n",
    "Every layer of every modern neural network does the same thing: **weighted sum → activation → repeat**.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next session, we will cover:\n",
    "- **Adaline:** Using gradient descent instead of the perceptron rule\n",
    "- **Backpropagation:** How to train multi-layer networks\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Interactive Demo](https://i33ym.cc/demo-perceptron/)\n",
    "- [Full Essay](https://i33ym.cc/the-perceptron/)\n",
    "- [Slides](https://i33ym.cc/slides-perceptron/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
