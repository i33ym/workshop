{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Production RAG\n",
    "\n",
    "In this notebook, we'll build a production-ready RAG system with:\n",
    "\n",
    "1. **Hybrid Search** — Vector + Keyword search combined\n",
    "2. **Reranking** — Score and filter results\n",
    "3. **Relevance Check** — Verify we found useful info\n",
    "4. **Query Rewrite** — Retry with better phrasing\n",
    "5. **Grounding Check** — Prevent hallucinations\n",
    "\n",
    "This is how you go from 30% to 86% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/i33ym/rag-workshop.git 2>/dev/null || echo \"Already cloned\"\n",
    "%cd rag-workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q openai langchain langchain-openai langchain-community chromadb rank-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Documents\n",
    "\n",
    "Same as Part 1 — load docs, split into chunks, create vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Load\n",
    "loader = DirectoryLoader(\"docs/\", glob=\"**/*.md\", loader_cls=TextLoader)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n## \", \"\\n### \", \"\\n\\n\", \"\\n\", \" \"]\n",
    ")\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "# Embed\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vector_store = Chroma.from_documents(documents=chunks, embedding=embeddings)\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "print(f\"Loaded {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Hybrid Search\n",
    "\n",
    "**Problem:** Vector search finds similar meanings but misses exact terms.\n",
    "\n",
    "**Solution:** Combine vector search + keyword search (BM25).\n",
    "\n",
    "### What is BM25?\n",
    "BM25 is a keyword search algorithm. It finds documents containing the exact words you searched for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "# Create BM25 retriever from same chunks\n",
    "bm25_retriever = BM25Retriever.from_documents(chunks)\n",
    "bm25_retriever.k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare: Vector vs BM25\n",
    "query = \"POST /api/payment\"\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "print(\"=== Vector Search Results ===\")\n",
    "vector_results = vector_store.similarity_search(query, k=3)\n",
    "for i, doc in enumerate(vector_results):\n",
    "    print(f\"{i+1}. {doc.page_content[:100]}...\\n\")\n",
    "\n",
    "print(\"=== BM25 (Keyword) Results ===\")\n",
    "bm25_results = bm25_retriever.invoke(query)\n",
    "for i, doc in enumerate(bm25_results[:3]):\n",
    "    print(f\"{i+1}. {doc.page_content[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reciprocal Rank Fusion (RRF)\n",
    "\n",
    "RRF combines results from multiple searches. Documents that appear in both searches rank higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query, k=5):\n",
    "    \"\"\"Combine vector search and BM25 using RRF.\"\"\"\n",
    "    \n",
    "    # Get results from both methods\n",
    "    vector_results = vector_store.similarity_search(query, k=k)\n",
    "    bm25_results = bm25_retriever.invoke(query)[:k]\n",
    "    \n",
    "    # Calculate RRF scores\n",
    "    rrf_scores = {}\n",
    "    k_constant = 60  # Standard RRF constant\n",
    "    \n",
    "    for rank, doc in enumerate(vector_results):\n",
    "        doc_id = doc.page_content[:100]  # Use content as ID\n",
    "        rrf_scores[doc_id] = rrf_scores.get(doc_id, 0) + 1 / (k_constant + rank + 1)\n",
    "        rrf_scores[doc_id + \"_doc\"] = doc  # Store the document\n",
    "    \n",
    "    for rank, doc in enumerate(bm25_results):\n",
    "        doc_id = doc.page_content[:100]\n",
    "        rrf_scores[doc_id] = rrf_scores.get(doc_id, 0) + 1 / (k_constant + rank + 1)\n",
    "        rrf_scores[doc_id + \"_doc\"] = doc\n",
    "    \n",
    "    # Sort by score and return documents\n",
    "    sorted_ids = sorted(\n",
    "        [k for k in rrf_scores.keys() if not k.endswith(\"_doc\")],\n",
    "        key=lambda x: rrf_scores[x],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    results = [rrf_scores[doc_id + \"_doc\"] for doc_id in sorted_ids[:k]]\n",
    "    scores = [rrf_scores[doc_id] for doc_id in sorted_ids[:k]]\n",
    "    \n",
    "    return results, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test hybrid search\n",
    "query = \"How do I authenticate API requests?\"\n",
    "\n",
    "results, scores = hybrid_search(query, k=5)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"=== Hybrid Search Results ===\")\n",
    "for i, (doc, score) in enumerate(zip(results, scores)):\n",
    "    print(f\"{i+1}. Score: {score:.4f}\")\n",
    "    print(f\"   {doc.page_content[:80]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Reranking\n",
    "\n",
    "**Problem:** Search returns documents by similarity, not by how well they answer the question.\n",
    "\n",
    "**Solution:** Score each document's relevance to the question and keep only the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def rerank_documents(query, documents, top_n=3):\n",
    "    \"\"\"Score each document's relevance and return top results.\"\"\"\n",
    "    \n",
    "    rerank_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Rate how relevant this document is to answering the question.\n",
    "Reply with only a number from 0 to 10.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Document: {document}\n",
    "\n",
    "Relevance score (0-10):\"\"\")\n",
    "    \n",
    "    chain = rerank_prompt | llm | StrOutputParser()\n",
    "    \n",
    "    scored_docs = []\n",
    "    for doc in documents:\n",
    "        try:\n",
    "            score_str = chain.invoke({\n",
    "                \"question\": query,\n",
    "                \"document\": doc.page_content[:500]\n",
    "            })\n",
    "            score = float(score_str.strip())\n",
    "        except:\n",
    "            score = 5.0\n",
    "        \n",
    "        scored_docs.append((doc, score))\n",
    "    \n",
    "    # Sort by score (highest first)\n",
    "    scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return scored_docs[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reranking\n",
    "query = \"How do I create a payment?\"\n",
    "\n",
    "# First, get hybrid results\n",
    "hybrid_results, _ = hybrid_search(query, k=6)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Reranking documents...\\n\")\n",
    "\n",
    "# Rerank them\n",
    "reranked = rerank_documents(query, hybrid_results, top_n=3)\n",
    "\n",
    "print(\"=== Reranked Results ===\")\n",
    "scores = []\n",
    "for i, (doc, score) in enumerate(reranked):\n",
    "    scores.append(score)\n",
    "    print(f\"{i+1}. Score: {score}/10\")\n",
    "    print(f\"   {doc.page_content[:80]}...\\n\")\n",
    "\n",
    "print(f\"\\nRerank scores: {scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: Relevance Check\n",
    "\n",
    "**Problem:** Sometimes no documents are truly relevant. Simple RAG generates an answer anyway.\n",
    "\n",
    "**Solution:** Check if documents can actually answer the question before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_relevance(query, documents):\n",
    "    \"\"\"Check if documents can answer the question.\"\"\"\n",
    "    \n",
    "    context = \"\\n\\n\".join([doc.page_content[:300] for doc in documents])\n",
    "    \n",
    "    relevance_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Can this context answer the question? Reply only 'yes' or 'no'.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Can this context answer the question?\"\"\")\n",
    "    \n",
    "    chain = relevance_prompt | llm | StrOutputParser()\n",
    "    result = chain.invoke({\"question\": query, \"context\": context})\n",
    "    \n",
    "    is_relevant = \"yes\" in result.lower()\n",
    "    return is_relevant, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a relevant question\n",
    "query = \"How do I authenticate?\"\n",
    "docs, _ = hybrid_search(query, k=3)\n",
    "\n",
    "is_relevant, raw = check_relevance(query, docs)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Is relevant: {is_relevant} (raw: {raw})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with an irrelevant question\n",
    "query = \"How do I integrate with PayPal?\"\n",
    "docs, _ = hybrid_search(query, k=3)\n",
    "\n",
    "is_relevant, raw = check_relevance(query, docs)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Is relevant: {is_relevant} (raw: {raw})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4: Query Rewrite\n",
    "\n",
    "**Problem:** Users don't always use the same terminology as the docs.\n",
    "\n",
    "**Solution:** If relevance check fails, rewrite the query and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query(query):\n",
    "    \"\"\"Rewrite query for better retrieval.\"\"\"\n",
    "    \n",
    "    rewrite_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Rewrite this question to be better for searching documentation.\n",
    "Use technical terms if applicable. Be specific.\n",
    "Reply with only the rewritten question.\n",
    "\n",
    "Original question: {question}\n",
    "\n",
    "Rewritten question:\"\"\")\n",
    "    \n",
    "    chain = rewrite_prompt | llm | StrOutputParser()\n",
    "    return chain.invoke({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query rewrite\n",
    "test_queries = [\n",
    "    \"How do I log in?\",\n",
    "    \"What happens when something goes wrong?\",\n",
    "    \"How do I get money from a customer?\"\n",
    "]\n",
    "\n",
    "for q in test_queries:\n",
    "    rewritten = rewrite_query(q)\n",
    "    print(f\"Original:  {q}\")\n",
    "    print(f\"Rewritten: {rewritten}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5: Grounding Check\n",
    "\n",
    "**Problem:** LLMs sometimes \"hallucinate\" — generate info not in the documents.\n",
    "\n",
    "**Solution:** Verify the answer is supported by the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_grounding(answer, documents):\n",
    "    \"\"\"Verify answer is supported by documents.\"\"\"\n",
    "    \n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "    \n",
    "    grounding_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Is this answer fully supported by the context? Reply only 'yes' or 'no'.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "Is the answer fully supported?\"\"\")\n",
    "    \n",
    "    chain = grounding_prompt | llm | StrOutputParser()\n",
    "    result = chain.invoke({\"context\": context, \"answer\": answer})\n",
    "    \n",
    "    is_grounded = \"yes\" in result.lower()\n",
    "    return is_grounded, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together: Production RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query, documents):\n",
    "    \"\"\"Generate answer from documents.\"\"\"\n",
    "    \n",
    "    context = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in documents])\n",
    "    \n",
    "    answer_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the question based only on the following context.\n",
    "If you can't find the answer, say \"I don't have information about this.\"\n",
    "Include code examples if relevant.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\")\n",
    "    \n",
    "    chain = answer_prompt | llm | StrOutputParser()\n",
    "    return chain.invoke({\"context\": context, \"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def production_rag(query, debug=False):\n",
    "    \"\"\"Full production RAG pipeline.\"\"\"\n",
    "    \n",
    "    debug_info = {\"query\": query}\n",
    "    \n",
    "    # Stage 1: Hybrid Search\n",
    "    if debug: print(\"[1] Hybrid Search...\")\n",
    "    docs, fusion_scores = hybrid_search(query, k=6)\n",
    "    debug_info[\"hybrid_results\"] = len(docs)\n",
    "    \n",
    "    # Stage 2: Reranking\n",
    "    if debug: print(\"[2] Reranking...\")\n",
    "    reranked = rerank_documents(query, docs, top_n=3)\n",
    "    rerank_scores = [score for _, score in reranked]\n",
    "    top_docs = [doc for doc, _ in reranked]\n",
    "    debug_info[\"rerank_scores\"] = rerank_scores\n",
    "    if debug: print(f\"    Scores: {rerank_scores}\")\n",
    "    \n",
    "    # Stage 3: Relevance Check\n",
    "    if debug: print(\"[3] Checking relevance...\")\n",
    "    is_relevant, _ = check_relevance(query, top_docs)\n",
    "    debug_info[\"is_relevant\"] = is_relevant\n",
    "    \n",
    "    # Stage 4: Query Rewrite (if not relevant)\n",
    "    if not is_relevant:\n",
    "        if debug: print(\"[4] Not relevant, rewriting query...\")\n",
    "        new_query = rewrite_query(query)\n",
    "        debug_info[\"rewritten_query\"] = new_query\n",
    "        if debug: print(f\"    Rewritten: {new_query}\")\n",
    "        \n",
    "        # Retry search\n",
    "        docs, _ = hybrid_search(new_query, k=6)\n",
    "        reranked = rerank_documents(new_query, docs, top_n=3)\n",
    "        top_docs = [doc for doc, _ in reranked]\n",
    "        \n",
    "        # Check relevance again\n",
    "        is_relevant, _ = check_relevance(new_query, top_docs)\n",
    "        if not is_relevant:\n",
    "            if debug: print(\"    Still not relevant. Giving up.\")\n",
    "            return {\n",
    "                \"answer\": \"I don't have information about this topic in the documentation.\",\n",
    "                \"debug\": debug_info\n",
    "            }\n",
    "    \n",
    "    if debug: print(f\"    Relevant: {is_relevant}\")\n",
    "    \n",
    "    # Stage 5: Generate Answer\n",
    "    if debug: print(\"[5] Generating answer...\")\n",
    "    answer = generate_answer(query, top_docs)\n",
    "    \n",
    "    # Stage 6: Grounding Check\n",
    "    if debug: print(\"[6] Checking grounding...\")\n",
    "    is_grounded, _ = check_grounding(answer, top_docs)\n",
    "    debug_info[\"is_grounded\"] = is_grounded\n",
    "    if debug: print(f\"    Grounded: {is_grounded}\")\n",
    "    \n",
    "    # Get sources\n",
    "    sources = list(set([doc.metadata.get(\"source\", \"unknown\") for doc in top_docs]))\n",
    "    \n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"sources\": sources,\n",
    "        \"is_grounded\": is_grounded,\n",
    "        \"debug\": debug_info\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Production Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with debug output\n",
    "result = production_rag(\"How do I authenticate API requests?\", debug=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ANSWER:\")\n",
    "print(\"=\"*50)\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiple questions\n",
    "test_questions = [\n",
    "    \"How do I create a payment?\",\n",
    "    \"What error codes can the API return?\",\n",
    "    \"How do I integrate with Stripe?\"  # Not in docs!\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(\"-\" * 40)\n",
    "    result = production_rag(q, debug=False)\n",
    "    print(f\"A: {result['answer'][:300]}...\")\n",
    "    print(f\"Grounded: {result.get('is_grounded', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare: Simple vs Production RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rag(query):\n",
    "    \"\"\"The simple approach from Part 1.\"\"\"\n",
    "    docs = vector_store.similarity_search(query, k=3)\n",
    "    return generate_answer(query, docs)\n",
    "\n",
    "# Compare on a tricky question\n",
    "query = \"How do I get a bearer token for authentication?\"\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "print(\"=== Simple RAG ===\")\n",
    "simple_answer = simple_rag(query)\n",
    "print(simple_answer[:300])\n",
    "\n",
    "print(\"\\n=== Production RAG ===\")\n",
    "prod_result = production_rag(query, debug=True)\n",
    "print(f\"\\nAnswer: {prod_result['answer'][:300]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we built:**\n",
    "\n",
    "| Stage | What it does | Why it matters |\n",
    "|-------|--------------|----------------|\n",
    "| Hybrid Search | Vector + BM25 | Catches both semantic and exact matches |\n",
    "| Reranking | Score relevance | Filters out noise |\n",
    "| Relevance Check | Verify we can answer | Prevents confident wrong answers |\n",
    "| Query Rewrite | Retry with better terms | Handles terminology mismatch |\n",
    "| Grounding Check | Verify answer is supported | Prevents hallucinations |\n",
    "\n",
    "**Benchmark improvement:**\n",
    "- Simple RAG: 0.30\n",
    "- Production RAG: 0.82-0.86\n",
    "\n",
    "**Next notebook:** Evaluation and debugging techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"✅ Part 2 complete!\")\n",
    "print(\"Next: Open 03_evaluation.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
