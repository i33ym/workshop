{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Evaluation & Debugging\n",
    "\n",
    "In this notebook, we'll learn how to:\n",
    "\n",
    "1. **Evaluate** RAG quality with test cases\n",
    "2. **Debug** when things go wrong\n",
    "3. **Improve** based on failures\n",
    "\n",
    "A system is only as good as your ability to measure and fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/i33ym/rag-workshop.git 2>/dev/null || echo \"Already cloned\"\n",
    "%cd rag-workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q openai langchain langchain-openai langchain-community chromadb rank-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load everything from Part 2\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Load and prepare\n",
    "loader = DirectoryLoader(\"docs/\", glob=\"**/*.md\", loader_cls=TextLoader)\n",
    "documents = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vector_store = Chroma.from_documents(documents=chunks, embedding=embeddings)\n",
    "bm25_retriever = BM25Retriever.from_documents(chunks)\n",
    "bm25_retriever.k = 5\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "print(f\"Loaded {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the pipeline functions from Part 2\n",
    "\n",
    "def hybrid_search(query, k=5):\n",
    "    vector_results = vector_store.similarity_search(query, k=k)\n",
    "    bm25_results = bm25_retriever.invoke(query)[:k]\n",
    "    \n",
    "    rrf_scores = {}\n",
    "    k_constant = 60\n",
    "    \n",
    "    for rank, doc in enumerate(vector_results):\n",
    "        doc_id = doc.page_content[:100]\n",
    "        rrf_scores[doc_id] = rrf_scores.get(doc_id, 0) + 1 / (k_constant + rank + 1)\n",
    "        rrf_scores[doc_id + \"_doc\"] = doc\n",
    "    \n",
    "    for rank, doc in enumerate(bm25_results):\n",
    "        doc_id = doc.page_content[:100]\n",
    "        rrf_scores[doc_id] = rrf_scores.get(doc_id, 0) + 1 / (k_constant + rank + 1)\n",
    "        rrf_scores[doc_id + \"_doc\"] = doc\n",
    "    \n",
    "    sorted_ids = sorted(\n",
    "        [k for k in rrf_scores.keys() if not k.endswith(\"_doc\")],\n",
    "        key=lambda x: rrf_scores[x], reverse=True\n",
    "    )\n",
    "    \n",
    "    results = [rrf_scores[doc_id + \"_doc\"] for doc_id in sorted_ids[:k]]\n",
    "    scores = [rrf_scores[doc_id] for doc_id in sorted_ids[:k]]\n",
    "    return results, scores\n",
    "\n",
    "def rerank_documents(query, documents, top_n=3):\n",
    "    rerank_prompt = ChatPromptTemplate.from_template(\n",
    "        \"Rate relevance 0-10. Reply with only a number.\\n\\nQuestion: {question}\\n\\nDocument: {document}\\n\\nScore:\"\n",
    "    )\n",
    "    chain = rerank_prompt | llm | StrOutputParser()\n",
    "    \n",
    "    scored = []\n",
    "    for doc in documents:\n",
    "        try:\n",
    "            score = float(chain.invoke({\"question\": query, \"document\": doc.page_content[:500]}).strip())\n",
    "        except:\n",
    "            score = 5.0\n",
    "        scored.append((doc, score))\n",
    "    \n",
    "    scored.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scored[:top_n]\n",
    "\n",
    "def check_relevance(query, documents):\n",
    "    context = \"\\n\\n\".join([doc.page_content[:300] for doc in documents])\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Can this context answer the question? Reply 'yes' or 'no'.\\n\\nQuestion: {question}\\n\\nContext: {context}\"\n",
    "    )\n",
    "    result = (prompt | llm | StrOutputParser()).invoke({\"question\": query, \"context\": context})\n",
    "    return \"yes\" in result.lower()\n",
    "\n",
    "def generate_answer(query, documents):\n",
    "    context = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in documents])\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Answer based only on context. If unsure, say so.\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "    )\n",
    "    return (prompt | llm | StrOutputParser()).invoke({\"context\": context, \"question\": query})\n",
    "\n",
    "def check_grounding(answer, documents):\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Is this answer supported by context? Reply 'yes' or 'no'.\\n\\nContext:\\n{context}\\n\\nAnswer: {answer}\"\n",
    "    )\n",
    "    result = (prompt | llm | StrOutputParser()).invoke({\"context\": context, \"answer\": answer})\n",
    "    return \"yes\" in result.lower()\n",
    "\n",
    "print(\"Pipeline functions loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Test Dataset\n",
    "\n",
    "To evaluate RAG, you need:\n",
    "1. **Questions** ‚Äî what users might ask\n",
    "2. **Expected answers** ‚Äî what the correct response should contain\n",
    "\n",
    "This is called a \"golden dataset\" or \"ground truth\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test cases\n",
    "test_cases = [\n",
    "    {\n",
    "        \"question\": \"How do I authenticate API requests?\",\n",
    "        \"expected_keywords\": [\"token\", \"authorization\", \"header\"],\n",
    "        \"should_answer\": True\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the endpoint for creating a payment?\",\n",
    "        \"expected_keywords\": [\"POST\", \"payment\", \"api\"],\n",
    "        \"should_answer\": True\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What error codes can the API return?\",\n",
    "        \"expected_keywords\": [\"error\", \"code\", \"400\", \"401\", \"500\"],\n",
    "        \"should_answer\": True\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do I integrate with Stripe?\",\n",
    "        \"expected_keywords\": [],\n",
    "        \"should_answer\": False  # Not in our docs!\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the meaning of life?\",\n",
    "        \"expected_keywords\": [],\n",
    "        \"should_answer\": False  # Completely off-topic\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Created {len(test_cases)} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "We'll measure three things:\n",
    "\n",
    "1. **Retrieval Quality** ‚Äî Did we find relevant documents?\n",
    "2. **Answer Quality** ‚Äî Does the answer contain expected information?\n",
    "3. **Appropriate Refusal** ‚Äî Did we correctly say \"I don't know\" when needed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answer(answer, test_case):\n",
    "    \"\"\"Evaluate a single answer against a test case.\"\"\"\n",
    "    \n",
    "    result = {\n",
    "        \"question\": test_case[\"question\"],\n",
    "        \"answer\": answer[:200],\n",
    "        \"metrics\": {}\n",
    "    }\n",
    "    \n",
    "    # Check if answer contains expected keywords\n",
    "    answer_lower = answer.lower()\n",
    "    \n",
    "    if test_case[\"should_answer\"]:\n",
    "        # Should provide an answer with keywords\n",
    "        keywords_found = sum(1 for kw in test_case[\"expected_keywords\"] if kw.lower() in answer_lower)\n",
    "        keywords_total = len(test_case[\"expected_keywords\"])\n",
    "        \n",
    "        if keywords_total > 0:\n",
    "            result[\"metrics\"][\"keyword_coverage\"] = keywords_found / keywords_total\n",
    "        else:\n",
    "            result[\"metrics\"][\"keyword_coverage\"] = 1.0\n",
    "            \n",
    "        # Check it's not a refusal\n",
    "        refusal_phrases = [\"don't have information\", \"cannot find\", \"no information\", \"i don't know\"]\n",
    "        is_refusal = any(phrase in answer_lower for phrase in refusal_phrases)\n",
    "        result[\"metrics\"][\"correctly_answered\"] = not is_refusal\n",
    "        \n",
    "    else:\n",
    "        # Should refuse to answer\n",
    "        refusal_phrases = [\"don't have information\", \"cannot find\", \"no information\", \"i don't know\"]\n",
    "        is_refusal = any(phrase in answer_lower for phrase in refusal_phrases)\n",
    "        result[\"metrics\"][\"correctly_refused\"] = is_refusal\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(test_cases, rag_function):\n",
    "    \"\"\"Run all test cases through the RAG system.\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, tc in enumerate(test_cases):\n",
    "        print(f\"Testing {i+1}/{len(test_cases)}: {tc['question'][:50]}...\")\n",
    "        \n",
    "        # Get answer\n",
    "        answer = rag_function(tc[\"question\"])\n",
    "        \n",
    "        # Evaluate\n",
    "        result = evaluate_answer(answer, tc)\n",
    "        results.append(result)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Simple vs Production RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple RAG function\n",
    "def simple_rag(query):\n",
    "    docs = vector_store.similarity_search(query, k=3)\n",
    "    return generate_answer(query, docs)\n",
    "\n",
    "# Production RAG function\n",
    "def production_rag(query):\n",
    "    # Hybrid search\n",
    "    docs, _ = hybrid_search(query, k=6)\n",
    "    \n",
    "    # Rerank\n",
    "    reranked = rerank_documents(query, docs, top_n=3)\n",
    "    top_docs = [doc for doc, _ in reranked]\n",
    "    \n",
    "    # Relevance check\n",
    "    if not check_relevance(query, top_docs):\n",
    "        return \"I don't have information about this topic in the documentation.\"\n",
    "    \n",
    "    # Generate\n",
    "    answer = generate_answer(query, top_docs)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"EVALUATING SIMPLE RAG\")\n",
    "print(\"=\" * 50)\n",
    "simple_results = run_evaluation(test_cases, simple_rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"EVALUATING PRODUCTION RAG\")\n",
    "print(\"=\" * 50)\n",
    "production_results = run_evaluation(test_cases, production_rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, tc in enumerate(test_cases):\n",
    "    print(f\"\\nQ: {tc['question']}\")\n",
    "    print(f\"Should answer: {tc['should_answer']}\")\n",
    "    print(f\"\\nSimple RAG:\")\n",
    "    print(f\"  {simple_results[i]['metrics']}\")\n",
    "    print(f\"Production RAG:\")\n",
    "    print(f\"  {production_results[i]['metrics']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging: When Things Go Wrong\n",
    "\n",
    "When RAG fails, you need to find where in the pipeline it broke:\n",
    "\n",
    "1. **Retrieval problem** ‚Äî Wrong documents retrieved\n",
    "2. **Reranking problem** ‚Äî Good docs scored low\n",
    "3. **Relevance problem** ‚Äî False positive/negative\n",
    "4. **Generation problem** ‚Äî Right docs, wrong answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_query(query):\n",
    "    \"\"\"Step through the pipeline and show what happens at each stage.\"\"\"\n",
    "    \n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Stage 1: Hybrid Search\n",
    "    print(\"\\n[STAGE 1: HYBRID SEARCH]\")\n",
    "    docs, scores = hybrid_search(query, k=6)\n",
    "    print(f\"Retrieved {len(docs)} documents\")\n",
    "    for i, (doc, score) in enumerate(zip(docs[:3], scores[:3])):\n",
    "        print(f\"  {i+1}. (score: {score:.4f}) {doc.page_content[:60]}...\")\n",
    "    \n",
    "    # Stage 2: Reranking\n",
    "    print(\"\\n[STAGE 2: RERANKING]\")\n",
    "    reranked = rerank_documents(query, docs, top_n=3)\n",
    "    rerank_scores = [score for _, score in reranked]\n",
    "    print(f\"Rerank scores: {rerank_scores}\")\n",
    "    top_docs = [doc for doc, _ in reranked]\n",
    "    for i, (doc, score) in enumerate(reranked):\n",
    "        print(f\"  {i+1}. (score: {score}/10) {doc.page_content[:60]}...\")\n",
    "    \n",
    "    # Stage 3: Relevance Check\n",
    "    print(\"\\n[STAGE 3: RELEVANCE CHECK]\")\n",
    "    is_relevant = check_relevance(query, top_docs)\n",
    "    print(f\"Is relevant: {is_relevant}\")\n",
    "    \n",
    "    if not is_relevant:\n",
    "        print(\"\\n‚ùå Pipeline stopped: Documents not relevant\")\n",
    "        return\n",
    "    \n",
    "    # Stage 4: Generate\n",
    "    print(\"\\n[STAGE 4: GENERATE ANSWER]\")\n",
    "    answer = generate_answer(query, top_docs)\n",
    "    print(f\"Answer: {answer[:300]}...\")\n",
    "    \n",
    "    # Stage 5: Grounding\n",
    "    print(\"\\n[STAGE 5: GROUNDING CHECK]\")\n",
    "    is_grounded = check_grounding(answer, top_docs)\n",
    "    print(f\"Is grounded: {is_grounded}\")\n",
    "    \n",
    "    if is_grounded:\n",
    "        print(\"\\n‚úÖ Pipeline complete: Answer is grounded\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Warning: Answer may contain hallucinations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug a successful query\n",
    "debug_query(\"How do I authenticate API requests?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug a query that should fail\n",
    "debug_query(\"How do I integrate with PayPal?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Failure Patterns\n",
    "\n",
    "### 1. Retrieval Failure\n",
    "The right documents aren't being found.\n",
    "\n",
    "**Symptoms:** Rerank scores are all low (< 5)\n",
    "\n",
    "**Fixes:**\n",
    "- Improve chunking (keep related content together)\n",
    "- Add metadata to help filtering\n",
    "- Tune BM25/vector weights in hybrid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Check if retrieval is the problem\n",
    "def diagnose_retrieval(query):\n",
    "    docs, _ = hybrid_search(query, k=6)\n",
    "    reranked = rerank_documents(query, docs)\n",
    "    scores = [s for _, s in reranked]\n",
    "    \n",
    "    avg_score = sum(scores) / len(scores)\n",
    "    max_score = max(scores)\n",
    "    \n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Scores: {scores}\")\n",
    "    print(f\"Average: {avg_score:.1f}, Max: {max_score}\")\n",
    "    \n",
    "    if max_score < 5:\n",
    "        print(\"‚ö†Ô∏è RETRIEVAL PROBLEM: No highly relevant docs found\")\n",
    "    elif avg_score < 4:\n",
    "        print(\"‚ö†Ô∏è PARTIAL PROBLEM: Some relevant docs, but noisy\")\n",
    "    else:\n",
    "        print(\"‚úÖ Retrieval looks good\")\n",
    "\n",
    "diagnose_retrieval(\"How do I create a payment?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. False Refusals\n",
    "The system says \"I don't know\" when the answer IS in the docs.\n",
    "\n",
    "**Symptoms:** Relevance check returns False incorrectly\n",
    "\n",
    "**Fixes:**\n",
    "- Adjust relevance threshold\n",
    "- Improve the relevance prompt\n",
    "- Check if chunking is splitting relevant content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what the relevance check sees\n",
    "def diagnose_relevance(query):\n",
    "    docs, _ = hybrid_search(query, k=6)\n",
    "    reranked = rerank_documents(query, docs, top_n=3)\n",
    "    top_docs = [doc for doc, _ in reranked]\n",
    "    \n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"\\nTop doc content:\")\n",
    "    print(top_docs[0].page_content[:500])\n",
    "    print(f\"\\nRelevance check result: {check_relevance(query, top_docs)}\")\n",
    "\n",
    "diagnose_relevance(\"How do I get an access token?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Hallucination\n",
    "The answer includes information not in the documents.\n",
    "\n",
    "**Symptoms:** Grounding check fails, or answer contains specific details not in context\n",
    "\n",
    "**Fixes:**\n",
    "- Strengthen the generation prompt\n",
    "- Lower temperature\n",
    "- Add explicit \"only use provided context\" instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for hallucination\n",
    "def diagnose_hallucination(query):\n",
    "    docs, _ = hybrid_search(query, k=6)\n",
    "    reranked = rerank_documents(query, docs, top_n=3)\n",
    "    top_docs = [doc for doc, _ in reranked]\n",
    "    \n",
    "    answer = generate_answer(query, top_docs)\n",
    "    is_grounded = check_grounding(answer, top_docs)\n",
    "    \n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"\\nAnswer: {answer[:300]}\")\n",
    "    print(f\"\\nGrounded: {is_grounded}\")\n",
    "    \n",
    "    if not is_grounded:\n",
    "        print(\"\\n‚ö†Ô∏è HALLUCINATION DETECTED\")\n",
    "        print(\"Check: Does the answer contain info not in the docs?\")\n",
    "\n",
    "diagnose_hallucination(\"How do I authenticate?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Feedback Loop\n",
    "\n",
    "The best RAG systems improve over time by collecting user feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple feedback collection\n",
    "feedback_log = []\n",
    "\n",
    "def rag_with_feedback(query):\n",
    "    \"\"\"RAG that collects feedback.\"\"\"\n",
    "    \n",
    "    # Get answer\n",
    "    answer = production_rag(query)\n",
    "    \n",
    "    # Log for review\n",
    "    entry = {\n",
    "        \"query\": query,\n",
    "        \"answer\": answer,\n",
    "        \"feedback\": None  # To be filled by user\n",
    "    }\n",
    "    feedback_log.append(entry)\n",
    "    \n",
    "    return answer, len(feedback_log) - 1  # Return answer and log ID\n",
    "\n",
    "def submit_feedback(log_id, is_helpful, comment=\"\"):\n",
    "    \"\"\"Submit feedback for an answer.\"\"\"\n",
    "    feedback_log[log_id][\"feedback\"] = {\n",
    "        \"helpful\": is_helpful,\n",
    "        \"comment\": comment\n",
    "    }\n",
    "    print(f\"Feedback recorded: {'üëç' if is_helpful else 'üëé'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "answer, log_id = rag_with_feedback(\"How do I create a payment?\")\n",
    "print(f\"Answer: {answer[:200]}...\")\n",
    "print(f\"Log ID: {log_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit feedback\n",
    "submit_feedback(log_id, is_helpful=True, comment=\"Clear and complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View feedback log\n",
    "import json\n",
    "print(json.dumps(feedback_log, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we learned:**\n",
    "\n",
    "1. **Test cases are essential** ‚Äî Define questions + expected answers\n",
    "2. **Measure at each stage** ‚Äî Find where failures happen\n",
    "3. **Common problems:**\n",
    "   - Retrieval failure ‚Üí Improve search/chunking\n",
    "   - False refusals ‚Üí Tune relevance check\n",
    "   - Hallucinations ‚Üí Strengthen generation prompt\n",
    "4. **Collect feedback** ‚Äî Improve over time\n",
    "\n",
    "**Key insight:** A debuggable system beats a clever system. Always know what's happening inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úÖ Workshop complete!\")\n",
    "print(\"\")\n",
    "print(\"You've learned:\")\n",
    "print(\"  1. Why simple RAG fails (30% accuracy)\")\n",
    "print(\"  2. How to build production RAG (86% accuracy)\")\n",
    "print(\"  3. How to evaluate and debug\")\n",
    "print(\"\")\n",
    "print(\"Next steps:\")\n",
    "print(\"  - Try with your own documents\")\n",
    "print(\"  - Build a web interface\")\n",
    "print(\"  - Add streaming responses\")\n",
    "print(\"  - Deploy to production\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
